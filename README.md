<h1 align="center">Welcome to kc-practica-bigdata-arquitecture </h1>
<p>
  <img alt="Version" src="https://img.shields.io/badge/version-1.0-blue.svg?cacheSeconds=2592000" />
  <a href="https://twitter.com/javiermiguelg">
    <img alt="Twitter: javiermiguelg" src="https://img.shields.io/twitter/follow/javiermiguelg.svg?style=social" target="_blank" />
  </a>
</p>

> Pr谩ctica de Arquitectura de BigData - Keepcoding 2019

## Estrategia

Servicio de informaci贸n de pisos y posibilidad de enfermar en ellos.

### Definici贸n resumen

- Descarga de Airbnb 
- Scrapeo www.sareb.es, idealista, fotocasa y otros con scripts en python/node.js corriendo en raspberries. Saco direcciones y precios.
- Recupero datasets de enfermedades del coraz贸n que contengan localizaci贸n (https://catalog.data.gov/dataset?tags=heart-disease, https://www.physionet.org/data/?topic=heart+disease&orderby=relevance-desc&types=0)
- Dise帽o un algoritmo que da un indicador de infarto en funci贸n de la zona geogr谩fica
- Aplico el algoritmo a todas las viviendas descargadas
- Monto HDFS/Yarn
- Cargo en Hive los datasets de pisos, enfermedades y localizaciones.
- Hago un API que le pasas un sitio, hace geocodificaci贸n con un API de terceros (de Google por ejemplo) y te da un indicador de posible infarto, precio de alquiler y precio de venta de vivienda de la zona, para que te decidas si te conviene vivir ah铆 :)
- Con rate-limiting controlar que las cuentas de prueba solo hacen unas pocas consultas y las de pago se limiten a la suscripci贸n comprada.
- Los logs los enchufo a un logstash que guarda la informaci贸n transaccional en ElasticSearch, con Kibana monto unos dashboards con KPIs del estatus de las raspberries, proceso etl, peticiones al API, errores, compras de suscripciones y peticiones de soporte.
- En el dashboard incluyo indicadores de sentimiento (con [sentiment](https://github.com/thisandagain/sentiment)) con respecto a mi marca/servicio en twitter/fb y posts en mis foros de soporte.
- Monto un proceso de actualizaci贸n que est谩 continuamente scrapeando enviando con kafka hacia storm que me alimenta el hive.



### Servicios

El catalogo de servicios proporcionado por el datalake se basa en informaci贸n de longevidad o posibilidad de enfermar en distintas partes del planeta en contraposici贸n de los precios de alquiler o venta en la zona.

### Datasets

Los datasets iniciales a incorporar son:
* Dataset de [Airbnb](https://public.opendatasoft.com/explore/dataset/airbnb-listings/export)
  * Se le aplicar谩 un proceso de limpieza de datos inservibles
* Datasets de enfermedades del coraz贸n que contengan localizaci贸n
  * Estos datsets ser谩n descargados de fuentes p煤blicas de informaci贸n
  * Se comenzar谩 con enfermedades del coraz贸n con datasets como:
    * [Datasets de data.gov](https://catalog.data.gov/dataset?tags=heart-disease)
    * [Datasets de Physionet](https://www.physionet.org/data/?topic=heart+disease&orderby=relevance-desc&types=0)
  * Posteriormente se ir谩n incluyendo m谩s datasets de enfermedades del coraz贸n como de otras enfermedades de las que tengamos localizaci贸n
* Datasets de portales de venta de viviendas como Idealista, Fotocasa, etc
  * Estos datasets ser谩n scrapeados



### Diagrama de Staging

Aqu铆 se muestran los [procesos](https://docs.google.com/drawings/d/1Ivb9oEZ5ztXWgYvPKWyj8zl0z1WF0_otQK8-3oKW6Uk/edit?usp=sharing) de staging.

<img alt="Staging" src="http://javiermiguel.com/images/misc/Practica%20JMG%20-%20Staging.jpg" />



## Arquitectura

A continuaci贸n se muestra un [diagrama](https://docs.google.com/drawings/d/1Yd_pnRy8ug83_h3uz-jg_c4py9ycoTn8pw20beO7Kqs/edit?usp=sharing) de la arquitectura de la soluci贸n.

<img alt="Staging" src="http://javiermiguel.com/images/misc/Practica%20JMG%20-%20DataLake.jpg" />

El modelo se desplegar谩 en la nube de Google Cloud.

### Componentes

Los componentes del sistema son:

* Hadoop HDFS
* YARN
* Hive
* Kafka
* Storm
* Logstash
* Elasticsearch
* Kibana
* API a consumidores



### Procesos

Los procesos definidos son:

* Descarga de ficheros.
  * Viviendas en alquiler - Airbnb
  * Enfermedades de coraz贸n por ubicaci贸n - data.gov, physionet, etc
* Web scraping de viviendas en venta - Idealista, Fotocasa, Sareb, etc
* Data quality. Cada dataset de entrada se beneficiar谩 de un proceso de calidad de datos:
  * Limpieza de registros que no cumplen el m铆nimo definido de informaci贸n
  * Normalizaci贸n de datos seg煤n la enciclopedia de tipos de datos
  * Deduplicaci贸n, se chequea la existencia de los nuevos datos que se pretenden incorporar para evitar tenerlos duplicados
  * Asignaci贸n de ubicaci贸n a nivel de coordenadas lat/lon. Para la informaci贸n relativa a areas extensas, se usar谩 el centroide del 谩rea
  * Proceso que calcula probabilidad de infarto en cada ubicaci贸n
* Carga en cluster por streaming
  * Los datos se emiten usando Kafka
  * Kafka se conecta a Storm para pasar los datos a HDFS
* Carga en Hive



## Modelo operativo

Primera Fase:

- La descarga de ficheros de airbnb, durante la 1陋 fase se hace a mano. En segunda fase de har谩 de forma automatizada con scripts en python/node.js.
- El proceso de web scraping de viviendas se programa para que est茅 de forma cont铆nua crwleando varios portales y scrapeando datos.
- Los procesos de data quality son invocados de forma autom谩tica ante cada fichero descargado.



Segunda Fase:

* Se van a帽adiendo otros datasets tanto de infartos como de otras enfermedades.

* Se automatiza la descarga de los ficheros de forma peri贸dica. Un script se encargar谩 de verificar cada d铆a si existen nuevas versiones de los datasets y en caso afirmativo lanzar谩 el script de descarga.




## Author

 **Javier Miguel**

* Twitter: [@javiermiguelg](https://twitter.com/javiermiguelg)
* Github: [@jamg44](https://github.com/jamg44)

## Show your support

Give a 猸锔 if this project helped you!
